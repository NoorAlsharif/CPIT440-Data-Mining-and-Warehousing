{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CPIT 440 lab manual - Lab 5  & Lab 6\n",
    " \n",
    "   ## Objectives\n",
    "   \n",
    "   These labs aim to practice the preprocessing steps to prepare the data to build the model. These steps will be as follwoing:  \n",
    "   1. Splitting into train and test sets\n",
    "   2. Divide the dataset into Dependent & Independent variable\n",
    "   3. Data cleaning (missing values)\n",
    "   4. Handling the categorical attributes\n",
    "   5. Handling outliers\n",
    "   6. Data transformation and scaling  \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#url=\"https://raw.githubusercontent.com/ageron/handson-ml/master/datasets/housing/housing.csv\"\n",
    "url=\"C:/Users/Asus/Desktop/IT/CPIT440/My Lab/My scripts/Lab 4 and 5 and 6/housing.csv\"\n",
    "housing=pd.read_csv(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: you can find the book and all its materials (dataset and the notebooks) in this link: https://github.com/ageron/handson-ml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Duplication:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes we find duplicates in our dataset which cause problems in the analysis. We need to remove the redundant rows.  \n",
    "  \n",
    "To print the duplicated rows in the dataset (this code only checks for duplicates but does not remove them):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.duplicated(subset=None, keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dup_rows = housing[housing.duplicated(subset=None, keep='first')]\n",
    "dup_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the above result, housing dataset has no duplicates.  \n",
    "  \n",
    "The following code can be used to find and drop all the duplicated rows in any dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "housing.drop_duplicates(subset=None, keep='first',inplace=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Sampling and Splitting into train and test sets\n",
    "\n",
    "The train-test split is a technique for evaluating the performance of a machine learning algorithm. It can be used for classification or regression problems and can be used for any supervised learning algorithm.  \n",
    "The procedure involves taking a dataset and dividing it into two subsets. The first subset is used to fit the model and is referred to as the training dataset. The second subset is not used to train the model; instead, the input element of the dataset is provided to the model, then predictions are made and compared to the expected values. This second dataset is referred to as the test dataset.  \n",
    "The objective is to estimate the performance of the machine learning model on new data: data not used to train the model.  \n",
    "The train-test procedure is appropriate when there is a sufficiently large dataset available that has suitable representation of the problem. A suitable representation of the problem domain means that there are enough records to cover all common cases and most uncommon cases in the domain. If you have insufficient data, then a suitable alternate model evaluation procedure would be the k-fold cross-validation procedure.   \n",
    "  \n",
    "Creating a test set is theoretically simple: just pick some instances **randomly**, typically 20% of the dataset, and set them aside. This is done to ensure that datasets are a representative sample (e.g. random sample) of the original dataset. We will need to set a variable called `random_state` to an integer value. Why? When comparing machine learning algorithms, it is desirable (perhaps required) that they are fit and evaluated on the same subsets of the dataset. This can be achieved by fixing the seed for the pseudo-random number generator used when splitting the dataset. This can be achieved by setting the `random_state` to an integer value. Any value will do; it is not a tunable hyperparameter. Moreover, when we repeat the run it is important that our random numbers generator, generates the same numbers for the test set to prevent our model from seeing all the dataset. This problem can be solved using the `random_state`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_set1, test_set1 = train_test_split(housing, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above method we used the **random sampling**. This is generally fine if your dataset is large enough (especially relative to the number of attributes), but if it is not, you run the risk of introducing a significant sampling bias. The test set should be a representative of the whole population. Sometimes, our data is skewed and we need to use **Stratified sampling** to take the same proportion from each part of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method `train_test_split` can be used to stratify by categorical variable. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose you chatted with experts who told you that the `median_income` is a very important attribute to predict median housing prices. You may want to ensure that the test set is representative of the various categories of incomes in the whole dataset. Since the median income is a continuous numerical attribute, you first need to create an income category attribute. Looking at the median income histogram more closely, most median income values are clustered around 2 to 5 (20,000–50,000) It is important to have a sufficient number of instances in your dataset for each stratum.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "housing.hist(column='median_income', bins=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code creates an income category attribute by multiplying the median income by (2/3) (to limit the number of income categories), and rounding up using ceil (to have discrete categories), and then keeping only the categories lower than 5 and merging the other categories into category 5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "housing[\"income_cat\"] = np.ceil(housing[\"median_income\"]*(2/3))\n",
    "housing[\"income_cat\"].where(housing[\"income_cat\"] < 5, 5.0, inplace=True)\n",
    "housing[\"income_cat\"].hist()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to apply the stratification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set2, test_set2 = train_test_split(housing, test_size=0.2, random_state=42,stratify=housing['income_cat'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing[\"income_cat\"].value_counts()/len(housing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set2[\"income_cat\"].value_counts()/len(train_set2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_set2[\"income_cat\"].value_counts()/len(test_set2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you should remove the income_cat attribute so the data is back to its original state:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set2=train_set2.drop(\"income_cat\", axis=1)\n",
    "test_set2=test_set2.drop(\"income_cat\", axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note in the following cell that the index of the rows is shuffled during the sampling process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can reset the index to start from 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set2.reset_index(drop=True, inplace=True)\n",
    "test_set2.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Divide the dataset into Dependent & Independent variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step would be to identify the independent variable (X or predictors) and the dependent variable (y, labels, or target). Assume that we want predict the house prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = train_set2.drop(\"median_house_value\", axis=1)\n",
    "train_y = train_set2[\"median_house_value\"].copy()\n",
    "test_X = test_set2.drop(\"median_house_value\", axis=1)\n",
    "test_y = test_set2[\"median_house_value\"].copy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The operations that are implemented in cleaning section and all the follwoing sections (section 4, 5, and 6) should be applied on both the train and the test sets. These steps are implemented after the spliting because we dont want to have any leakage from the test set to the train set when we calculate some central tendency measures. Data leakage happens when we create our model using information from outside the training dataset. This can produce optimistic and overfitted model that can fail to predict the unseen samples. Hence, the first step in the preprocessing should be splitting the data into train and test sets.   \n",
    "To read more about data leakage: https://machinelearningmastery.com/data-leakage-machine-learning/ \n",
    "  \n",
    "In the following cells I will apply the cleaning on the train set. The same steps should be repeated on the test set.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing=train_X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing values\n",
    "\n",
    "Most Machine Learning algorithms cannot work with missing features, so let’s create a few functions to take care of them. You noticed earlier that the total_bedrooms attribute has some missing values, so let’s fix this.  \n",
    "You have three options:  \n",
    "* option 1: Get rid of the corresponding districts.\n",
    "* option 2: Get rid of the whole attribute.\n",
    "* option 3: Set the values to some value (zero, the mean, the median, etc.).  \n",
    "\n",
    "\n",
    "First we will check for the null values: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The following two commnads show the missing values\n",
    "housing.info()\n",
    "housing.isnull().sum(axis=0)  # sum of rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To handle the missing values:\n",
    "\n",
    "```python\n",
    "# option 1\n",
    "housing.dropna(subset=[\"total_bedrooms\"],inplace=True) \n",
    "# option 2\n",
    "housing.drop(\"total_bedrooms\", axis=1,inplace=True) \n",
    "# option 3\n",
    "median = housing[\"total_bedrooms\"].median() \n",
    "housing[\"total_bedrooms\"].fillna(median, inplace=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Practice**  \n",
    "The median cannot be calculated for the categorical attributes. If we have missing values in the attribute `ocean_proximity`, what is the suitable central tendency measure to use? Can you change the code in `# option 3` so it can handle categorical attribute? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An alternative method to fill the missing values is to use a class in Scikit-Learn to take care of missing values: `SimpleImputer`. First, you need to create a `SimpleImputer` instance, specifying that you want to replace each attribute’s missing values with the median of that attribute. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "import numpy as np\n",
    "imputer = SimpleImputer(missing_values=np.nan,strategy='median')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the median can only be computed on numerical attributes, we need to create a copy of the data without categorical attribute `ocean_proximity`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_num = housing.drop(\"ocean_proximity\", axis=1)\n",
    "housing_num"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can fit the imputer instance to the data using the `fit()` method. In this method, the imputer computes the median of each attribute and stores the result in its `statistics_` instance variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer.fit(housing_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer.statistics_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_num.median().values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can use this trained imputer to transform the data set by replacing missing values by the learned medians."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = imputer.transform(housing_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is a NumPy array containing the transformed features. If you want to put it back into a Pandas DataFrame, it’s simple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_tr = pd.DataFrame(X, columns=housing_num.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_tr.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We finished the cleaning of the train set, but we droped the atrribute `ocean_proximity`. In the following section we will encode this categorical attribute before we restore it to our train set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 - Handeling the categorical attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our data we have one categorical attribute `ocean_proximity`. Most Machine Learning algorithms prefer to work with numbers, since the models are based on mathematical equations and calculations. So let’s convert these categories from text to numbers. For this, we can use Pandas’ `factorize()` method which maps each category to a different integer.  \n",
    "  \n",
    "**Note:** The method explained in this section is suitable to categorical attributes in the input space `X`. If you work in classification task and want to encode the output `y`, you can use the class `LabelEncoder`. Moreover, the method is suitable for the nominal attributes. If you have an ordinal attribute in the input `X`, you can use the class `OrdinalEncoder`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_cat = housing[\"ocean_proximity\"]\n",
    "housing_cat_encoded, housing_categories = housing_cat.factorize()\n",
    "print('housing_categories: ',housing_categories)\n",
    "print('housing_cat_encoded: ',housing_cat_encoded)\n",
    "print('housing_cat_encoded shape: ',housing_cat_encoded.shape)\n",
    "type(housing_cat_encoded)   #it is a vector (one-dimensional structure)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One issue with this representation is that ML algorithms will assume that two nearby\n",
    "values are more similar than two distant values. Obviously this is not the case. To fix this issue, a common solution is to create one binary attribute per category. I.e. convert the nominal attribute to many asymmetric binary attributes.  This is called one-hot encoding.  \n",
    "  \n",
    "  \n",
    "**Number of columns = states of the categorical attribute**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-Learn provides a `OneHotEncoder` encoder to convert integer categorical values\n",
    "into one-hot vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "encoder = OneHotEncoder()\n",
    "housing_cat_1hot = encoder.fit_transform(housing_cat_encoded.reshape(-1,1))\n",
    "housing_cat_1hot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that `fit_transform()` expects a 2D array, but `housing_cat_encoded` is a 1D array, so we need to reshape it. Also, notice that the output is a SciPy sparse matrix, instead of a NumPy array. This is very useful when you have categorical attributes with thousands of categories. After one-hot encoding we get a matrix with thousands of columns, and the matrix is full of zeros except for a single 1 per row. Using up tons of memory mostly to store zeros would be very wasteful, so instead a sparse matrix only stores the location of the nonzero elements. You can use it mostly like a normal 2D array, but if you really want to convert it to a (dense) NumPy array, just call the `toarray()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_cat_1hot=housing_cat_1hot.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(housing_cat_1hot))\n",
    "print(housing_cat_1hot[0:20,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will convert `housing_cat_1hot` to a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_cat =pd.DataFrame(housing_cat_1hot,columns=housing_categories.tolist()) #contains categorical attributes\n",
    "housing_cat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By the end of section 3 we got `housing_tr` which has numerical attributes without missing values. Also, by the end of section 4 we got `housing_cat` which has the encoded categorical attributes.  \n",
    "We should concatenate them, however, first we need to check outliers and do scaling which will be done on the numerical attributes before concatenation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 - Handlig outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In statistics, an outlier is an observation point that is distant from other observations. The outliers can be a result of a mistake during data collection or it can be just an indication of variance in your data. Outliers can skew statistical measures and data distributions which produce bad prediction and analysis of the data.   \n",
    "There are many methods to detect outliers in the dataset. Some of them are simple statistical methods such as methods that use standard deviations or the interquartile range. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following we will apply the IQR (interquartile range) method.  \n",
    "First, we will calculate the IQR of all the __numerical attributes__:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1 = housing_tr.quantile(0.25)\n",
    "Q3 = housing_tr.quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "IQR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier = ( housing_tr.min() < Q1-1.5*IQR) | (housing_tr.max() > Q3+1.5*IQR)\n",
    "outlier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above cell shows that 5 attributes have outliers.  \n",
    "  \n",
    "If we decided to remove all the rows that include outliers, we can use the code in the following cells:  \n",
    "\n",
    "\n",
    "In the following code, we make boolean series such that the row that has outlier will be True. `DataFrame.any()` returns whether any element is True over requested axis.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#all the rows that has outliers will be True\n",
    "outlier_index = ((housing_tr < (Q1 - 1.5 * IQR)) |(housing_tr > (Q3 + 1.5 * IQR))).any(axis=1)\n",
    "\n",
    "outlier_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To print the number of rows that include outlier in any column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(outlier_index.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell, we will print only the outlier rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_tr[outlier_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The follwoing code will print the index of the outlier rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_numbers = housing_tr[outlier_index].index\n",
    "index_numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will drop all the rows that have outlier values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Either using the method drop as follwoing\n",
    "housing_out=housing_tr.drop(index_numbers)\n",
    "\n",
    "# or using the following code\n",
    "#housing_out = housing_tr[~outlier_index]\n",
    "housing_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A final thing we should mention here is that the above method is very simple and not adequate when we have high-dimensional input feature space. We will keep these outliers in our dataset and see the accuracy of the models. If you need more robust analysis of outliers, you can look at the following resources:  \n",
    "1. Jiawei Han, Micheline Kamber and Jian Pei, \"Data Mining Concepts and Techniques\", Morgan Kaufmann Publishers, Third Edition, 2012. __Chapter 12: Outlier detection__\n",
    "2. https://machinelearningmastery.com/model-based-outlier-detection-and-removal-in-python/. This site explains how to use `scikit-learn` library to find outliers using various robust methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Data transformation and scaling "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the most important transformations you need to apply to your data is feature scaling. With few exceptions, Machine Learning algorithms don’t perform well when the input numerical attributes have very different scales. This is the case for the housing data: the total number of rooms ranges from about 6 to 39,320, while the median incomes only range from 0 to 15. **Note that scaling the target values is generally not required**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two common ways to get all attributes to have the same scale: min-max scaling and standardization (z-score normalization). Unlike min-max scaling, standardization does not bound values to a specific range, which may be a problem for some algorithms (e.g., neural networks often expect an input value ranging from 0 to 1). However, standardization is much less affected by outliers.  \n",
    "Scikit-Learn provides a transformer called `StandardScaler` for standardization.  \n",
    "It is important to fit the scalers to the training data only, not to the full dataset (including the test set). Only then can you use them to transform the training set and the test set (and new data). We will apply the scaling on the numerical features which are stored in `housing_tr` produced in section 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(housing_tr)\n",
    "scaled=scaler.transform(housing_tr)\n",
    "scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is a NumPy array containing the transformed features. If you want to put it back into a Pandas DataFrame, it’s simple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_scaled = pd.DataFrame(scaled, columns=housing_tr.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "housing_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will merge the two dataframes (housing_scaled and housing_cat) in one dataframe to have all the numerical and categorical attributes in one dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_train = pd.concat([housing_scaled, housing_cat], axis=1)\n",
    "housing_train\n",
    "#housing.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we will export the train dataset to `csv` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_train.to_csv('train_X.csv',index=False)\n",
    "train_y.to_csv('train_y.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the follwoing cell I will repeat the steps to be implemented on the test set. Note that I will take the imputer, encoder, and scaler as fitted from the train data and use them to transform the test set to prevent data leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#missing values\n",
    "housing = test_X\n",
    "housing_num = housing.drop(\"ocean_proximity\", axis=1)\n",
    "X = imputer.transform(housing_num)\n",
    "housing_tr = pd.DataFrame(X, columns=housing_num.columns)\n",
    "#categorical attribute\n",
    "housing_cat = housing[\"ocean_proximity\"]\n",
    "housing_cat_encoded, housing_categories = housing_cat.factorize()\n",
    "housing_cat_1hot = encoder.transform(housing_cat_encoded.reshape(-1,1))\n",
    "housing_cat_1hot=housing_cat_1hot.toarray()  #from sparse to dense\n",
    "housing_cat =pd.DataFrame(housing_cat_1hot,columns=housing_categories.tolist()) \n",
    "#standardization\n",
    "scaled=scaler.transform(housing_tr)\n",
    "housing_scaled = pd.DataFrame(scaled, columns=housing_tr.columns)\n",
    "#concatenation of numerical and categorical\n",
    "housing_test = pd.concat([housing_scaled, housing_cat], axis=1)\n",
    "#export to CSV files\n",
    "housing_test.to_csv('test_X.csv',index=False)\n",
    "test_y.to_csv('test_y.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References:\n",
    "1. “Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems”, Aurélien Géron,O'Reilly Media; 2 edition (August 4, 2019)  \n",
    "2. https://machinelearningmastery.com/model-based-outlier-detection-and-removal-in-python/\n",
    "3. https://towardsdatascience.com/ways-to-detect-and-remove-the-outliers-404d16608dba"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
