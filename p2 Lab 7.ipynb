{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CPIT 440 lab manual - Lab 7\n",
    " \n",
    "   ## Objective\n",
    "   \n",
    "   This lab aims to:\n",
    "   1. Select and train a regression model \n",
    "   2. Measure the performance of the model (underfitting, overfitting)\n",
    "   3. Better Evaluation Using Cross-Validation\n",
    "   4. Random Forest and Ensemble learning\n",
    "   5. Evaluate Your System on the Test Set\n",
    "     \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Select and train a regression model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous two labs we applied some preprocessing steps on the housing dataset. By the end of these steps, we prepared our data and it is ready to train a model. The output space is the `median_hous_value`, which is a continous feature, hence, we will use regression.  \n",
    "In the following cell we will read the prepared train set (input and output)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "url=\"C:/Users/9SAD/Desktop/IT/CPIT 440/My labs/Lab 7/train_X.csv\"\n",
    "housing=pd.read_csv(url)\n",
    "url=\"C:/Users/9SAD/Desktop/IT/CPIT 440/My labs/Lab 7/train_y.csv\"\n",
    "housing_labels=pd.read_csv(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s first train a Linear Regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(housing, housing_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`lin_reg` is our model. Let’s try the model on a few instances from the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: [[211574.39523833]\n",
      " [321345.10513719]\n",
      " [210947.519838  ]\n",
      " [ 61921.01197837]\n",
      " [192362.32961119]]\n"
     ]
    }
   ],
   "source": [
    "some_data = housing.iloc[:5]\n",
    "some_labels = housing_labels.iloc[:5]\n",
    "print(\"Predictions:\", lin_reg.predict(some_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels:    median_house_value\n",
      "0            286600.0\n",
      "1            340600.0\n",
      "2            196900.0\n",
      "3             46300.0\n",
      "4            254500.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Labels:\", some_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Measure the performance of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It works, although the predictions are not exactly accurate. We need to measure the performance of the regression model. To do, we will calculate the distance between two vectors: the vector of the actual labels, and the vector of the predicted labels. The distance can be measured using many methods:  \n",
    "1. Computing the root of a sum of squares (Root Mean Square Error=RMSE) corresponds to the Euclidean norm\n",
    "2. Computing the sum of absolutes (Mean Absolute Error=MAE) corresponds to Manhattan norm\n",
    "3. Minkowski distance to have general k norm index. **The higher the norm index, the more it focuses on large values and neglects small ones.** This is why the RMSE is more sensitive to outliers than the MAE.  \n",
    "Let’s measure this regression model’s RMSE on the whole training set using Scikit-Learn’s mean_squared_error function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69050.98178244587"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "housing_predictions = lin_reg.predict(housing)\n",
    "lin_mse = mean_squared_error(housing_labels, housing_predictions)\n",
    "lin_rmse = np.sqrt(lin_mse)\n",
    "lin_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The prediction error is $69,050 is not very satisfying. This is an example of a model **underfitting** the training data. When this happens it can mean that the features do not provide enough information to make good predictions, or that the model is not powerful enough."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main ways to fix underfitting are to select a more powerful model or to feed the training algorithm with better features.  \n",
    "let’s try a more complex model to see how it does. Let’s train a DecisionTreeRegressor. This is a powerful model, capable of finding complex nonlinear relationships in the data, while the linear regression model can find only the linear relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "tree_reg = DecisionTreeRegressor(random_state=42)\n",
    "tree_reg.fit(housing, housing_labels)\n",
    "housing_predictions = tree_reg.predict(housing)\n",
    "tree_mse = mean_squared_error(housing_labels, housing_predictions)\n",
    "tree_rmse = np.sqrt(tree_mse)\n",
    "tree_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No error at all! Could this model really be absolutely perfect? Of course, it is much more likely that the model has badly **overfit** the data. How can you be sure? As we saw earlier, you don’t want to touch the test set until you are ready to launch a model you are confident about, so you need to use part of the training set for training, and part for model validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Better Evaluation Using Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to evaluate the Decision Tree model is to use cross-validation. It randomly splits the training set into 10 (or generally k) distinct subsets called folds, then it trains and evaluates the Decision Tree model 10 times, picking a different fold for evaluation every time and training on the other 9 folds. The result is an array containing the 10 evaluation scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "output = cross_validate(tree_reg, housing, housing_labels, scoring=\"neg_mean_squared_error\", cv=10, return_estimator =True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameter `scoring` controls what metric will be applied to evaluate the estimators. More options of this parameter can be found in this link: https://scikit-learn.org/stable/modules/model_evaluation.html  \n",
    "The return of `cross_validate` is dict type that is composed of many keys. Each key corresponds to a float array that describe the scores of the estimator for each run of the cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "dict_keys(['fit_time', 'score_time', 'estimator', 'test_score'])\n"
     ]
    }
   ],
   "source": [
    "print(type(output))\n",
    "print(output.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_rmse_scores = np.sqrt(-output[\"test_score\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-Learn cross-validation features expect a utility function (greater is better) rather than a cost function (lower is better), so the scoring function is actually the opposite of the MSE (i.e., a negative value), which is why the preceding code computes -scores before calculating the square root.  \n",
    "The results are as the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_scores(scores):\n",
    "    print(\"Scores:\", scores)\n",
    "    print(\"Mean:\", scores.mean())\n",
    "    print(\"Standard deviation:\", scores.std())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores: [66757.2013883  67056.96957579 71135.77592971 69317.58307062\n",
      " 68729.37098167 75888.88658511 67043.83260349 70251.84122452\n",
      " 69273.49135944 69161.37598811]\n",
      "Mean: 69461.63287067825\n",
      "Standard deviation: 2536.954810668593\n"
     ]
    }
   ],
   "source": [
    "display_scores(tree_rmse_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the Decision Tree doesn’t look as good as it did earlier. In fact, it seems to perform worse than the Linear Regression model! Notice that cross-validation allows you to get not only an estimate of the performance of your model, but also a measure of how precise this estimate is (i.e., its standard deviation). The Decision Tree has a score of approximately 69461, generally ±2536. You would not have this information if you just used one validation set. But cross-validation comes at the cost of training the model several times, so it is not always possible.  \n",
    "Let’s compute the same scores for the Linear Regression model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores: [67450.42057782 67329.50264436 68361.84864912 74639.88837894\n",
      " 68314.56738182 71628.61410355 65361.14176205 68571.62738037\n",
      " 72476.18028894 68098.06828865]\n",
      "Mean: 69223.18594556303\n",
      "Standard deviation: 2657.268311277696\n"
     ]
    }
   ],
   "source": [
    "lin_output = cross_validate(lin_reg, housing, housing_labels, scoring=\"neg_mean_squared_error\", cv=10, return_estimator =True)\n",
    "lin_rmse_scores = np.sqrt(-lin_output[\"test_score\"])\n",
    "display_scores(lin_rmse_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is clear that the Decision Tree model is overfitting so badly that it performs worse than the Linear Regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Importance of each attribute**  \n",
    "Some of the estimators (such as DecisionTreeRegressor) can indicate the relative importance of each attribute for making accurate predictions. In the following code we will extract the `feature_importances_` from each estimator. We have one estimator per each fold. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.11483581e-01 1.10018495e-01 4.90616298e-02 2.37806627e-02\n",
      " 1.94510714e-02 3.15252868e-02 2.15176275e-02 4.80388744e-01\n",
      " 2.45716291e-03 6.77085356e-03 1.42555214e-01 9.60299365e-04\n",
      " 2.93720276e-05]\n"
     ]
    }
   ],
   "source": [
    "feature_importances=np.zeros(len(housing.columns)) \n",
    "for estimator in output['estimator']:\n",
    "    feature_importances=np.add(feature_importances,estimator.feature_importances_)\n",
    "\n",
    "feature_importances=np.divide(feature_importances,10)   # because cv=10, hence we calculate the mean\n",
    "print(feature_importances)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s display these importance scores next to their corresponding attribute names. They are sorted from the highest one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.48038874367467993, 'median_income'),\n",
       " (0.14255521430539606, 'INLAND'),\n",
       " (0.11148358105795102, 'longitude'),\n",
       " (0.1100184949333836, 'latitude'),\n",
       " (0.04906162975679831, 'housing_median_age'),\n",
       " (0.031525286787880594, 'population'),\n",
       " (0.023780662729606352, 'total_rooms'),\n",
       " (0.02151762751553365, 'households'),\n",
       " (0.01945107137708847, 'total_bedrooms'),\n",
       " (0.006770853558282363, 'NEAR OCEAN'),\n",
       " (0.0024571629105240786, '<1H OCEAN'),\n",
       " (0.0009602993653243291, 'NEAR BAY'),\n",
       " (2.9372027551157247e-05, 'ISLAND')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(zip(feature_importances, housing.columns), reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Random Forests and Ensemble Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Random Forests** work by training many Decision Trees on random subsets of the features, then averaging out their predictions. Building a model on top of many other models is called **Ensemble Learning**, and it is often a great way to push ML algorithms even further.  \n",
    "We will apply the random forests using RandomForestRegressor from sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* First we will train the model using all the train set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(housing_labels.values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(n_estimators=10)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "forest_reg = RandomForestRegressor(n_estimators=10 )  #n_estimators is the number of trees, default is 100\n",
    "forest_reg.fit(housing, housing_labels.values.ravel())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: The random forests expect y (housing labels) to be a 1-dimensional array instead of column vector, hence, we used `housing_labels.values.ravel()` to make this conversion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will evaluate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22290.181339519884"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "housing_predictions = forest_reg.predict(housing)\n",
    "forest_mse = mean_squared_error(housing_labels, housing_predictions)\n",
    "forest_rmse = np.sqrt(forest_mse)\n",
    "forest_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although that the score is very promising, however, we cannot be sure. It is possible that we have overfitting. We will use the cross-validation to make sure.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores: [50018.07451035 48243.80974457 51981.60184227 51841.23532902\n",
      " 52667.29018291 54944.96316004 50984.61061139 53138.56292312\n",
      " 55491.41588886 52011.68980642]\n",
      "Mean: 52132.32539989714\n",
      "Standard deviation: 2037.7189546907696\n"
     ]
    }
   ],
   "source": [
    "\n",
    "forest_output = cross_validate(forest_reg, housing, housing_labels.values.ravel(), scoring=\"neg_mean_squared_error\", cv=10,  return_estimator =True)\n",
    "forest_rmse_scores = np.sqrt(-forest_output[\"test_score\"])\n",
    "display_scores(forest_rmse_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It has better score than the Decision tree but not that much. Note that the score on the training set is still much lower than on the validation sets, meaning that the model is still overfitting the training set. Possible solutions for overfitting are to constrain the model (i.e., regularize it), or get a lot more training data. Or you should try out many other models from various categories of Machine Learning algorithms (several Support Vector Machines with different kernels, possibly a neural network, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will calculate the importance of each attribute in the same way. The estimator here is `RandomForestRegressor`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.05856981e-01 1.04571001e-01 5.24519218e-02 2.47244992e-02\n",
      " 2.22491182e-02 3.25152219e-02 2.14907818e-02 4.82560176e-01\n",
      " 3.45931992e-03 6.11512192e-03 1.43035686e-01 9.26181680e-04\n",
      " 4.39900713e-05]\n"
     ]
    }
   ],
   "source": [
    "feature_importances=np.zeros(len(housing.columns)) \n",
    "for estimator in forest_output['estimator']:\n",
    "    feature_importances=np.add(feature_importances,estimator.feature_importances_)\n",
    "\n",
    "feature_importances=np.divide(feature_importances,10)   # because cv=10, hence we calculate the mean\n",
    "print(feature_importances)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s display these importance scores next to their corresponding attribute names. They are sorted from the highest one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.4825601759355641, 'median_income'),\n",
       " (0.14303568558515292, 'INLAND'),\n",
       " (0.10585698075882041, 'longitude'),\n",
       " (0.10457100112527054, 'latitude'),\n",
       " (0.05245192181797348, 'housing_median_age'),\n",
       " (0.032515221881812155, 'population'),\n",
       " (0.0247244992453355, 'total_rooms'),\n",
       " (0.022249118245752972, 'total_bedrooms'),\n",
       " (0.021490781808945137, 'households'),\n",
       " (0.00611512192264683, 'NEAR OCEAN'),\n",
       " (0.003459319921091613, '<1H OCEAN'),\n",
       " (0.0009261816803729428, 'NEAR BAY'),\n",
       " (4.399007126143354e-05, 'ISLAND')]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(zip(feature_importances, housing.columns), reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can note that the Random Forest Regressor produce feature_importance that is slightly different from the Decision Tree Regressor, although they are approximately very close. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Evaluate Your System on the Test Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After tweaking your models for a while, you eventually have a system that performs sufficiently well. Now is the time to evaluate the final model on the test set.  \n",
    "First we will read the test set:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "url=\"C:/Users/Asus/Desktop/IT/CPIT440/My Lab/My scripts/Lab 4 and 5 and 6/test_X.csv\"\n",
    "test_X=pd.read_csv(url)\n",
    "url=\"C:/Users/Asus/Desktop/IT/CPIT440/My Lab/My scripts/Lab 4 and 5 and 6/test_y.csv\"\n",
    "test_y=pd.read_csv(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80133.26540456581"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_predictions = forest_reg.predict(test_X)\n",
    "final_mse = mean_squared_error(test_y, final_predictions)\n",
    "final_rmse = np.sqrt(final_mse)\n",
    "final_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance will usually be slightly worse than what you measured using cross-validation. This means that we need to alter the hyperparameters of the estimator until we find good results on the test set. However, sometimes when this happens, it means that our model is not mature or not suitable. As we said previously, possible solutions are to constrain the model (i.e., regularize it), or get a lot more training data. Or you should try out many other models from various categories of Machine Learning algorithms (several Support Vector Machines with different kernels, possibly a neural network, etc.)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Assignment\n",
    "In the housing dataset, we detected some problems such as capping the `median_house_value` at 500001. In addition, we did not remove the outliers.  \n",
    "1. remove all the districts whose price equals 500001\n",
    "2. remove outliers\n",
    "3. repeat all the preprocessing steps\n",
    "4. use the `RandomForestRegressor` again and use the cross validation\n",
    "5. what is the importance of each feature based on the cross validation\n",
    "6. evaluate the model on on the test set, and comment on the new score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
